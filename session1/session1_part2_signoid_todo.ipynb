{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MQFhwaLZZjoUiCPLUzGt8PwzBU7L7FQF","timestamp":1666458290528}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TMI8P3Rr6f22"},"source":["This is the second part of the first AI session!  \n","Here you will code a **sigmoid neuron**."]},{"cell_type":"code","metadata":{"id":"qO21nyMQDc74"},"source":["#Some useful imports\n","import math\n","from pylab import *\n","import matplotlib.pyplot as plt\n","from random import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xWn39ERTX4CZ"},"source":["## **Recap from the presentation**\n","\n","###1. Sigmoid\n","The **sigmoid activation function** that we will note $\\sigma$ is:  \n","$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$  \n","And we have :  \n","$$\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$$\n","$$\\\\[0.2 cm]$$\n","###2. Loss function\n","Let's consider:  \n","$~~~~~$-$\\hat{y_k}$ the value to predict for the k-th training.\n","\n","$~~~~~$-$y_k$ The predicted value by the model for the k-th training.\n","\n","$~~~~~$-n the number of training samples.\n","\n","The **loss function** is the following:\n","$$L = \\frac{1}{n}\\sum_{k =1}^n(\\hat{y_k} - y_k)^2$$\n","$$\\\\[0.4 cm]$$\n","\n","*$\\rightarrow$How to compute $\\frac{\\partial L}{\\partial \\omega_i}$ for the k-th input?*\n","\n","Let's consider $h_k$, the output of the perceptron, and $y_k$ after the sigmoid function:  \n","\n","$$\n","h_k = \\sum_{i} a_{i}^{k}*w_{i} + b \\\\  \n","y_k = \\sigma(h_k)\n","$$\n","\n","Using the *chain rule*,\n","$$\\frac{\\partial L}{\\partial \\omega_i} = \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k}\\frac{\\partial h_k}{\\partial w_i}$$\n","\n","we have:\n","$$\\frac{\\partial L}{\\partial \\omega_i} = -2(\\hat{y_i} - y_i) \\sigma'(h_k)a_{i}^k$$\n","\n","To simplify the calculations during the execution of the algorithm we will note:\n","\n","$$\\delta  = -2(\\hat{y_i} - y_i)\\sigma'(h_k)$$\n","\n","and we get: $$\\frac{\\partial L}{\\partial \\omega_i} = \\delta a_{i}^k$$\n","\n","###3. Updating\n","\n","Let's consider $w^{(n)}$ and $b^{(n)}$ the walues of the weights and the bias at step n.\n","\n","In order to update the weights and biases, we use the following equation:\n","\n","$$\\forall i, \\ \\ \\ w_i^{(n+1)} = w_i^{(n)} - η\\frac{\\partial L}{\\partial \\omega_i}$$\n","$$b^{(n+1)} = b^{(n)} - η\\frac{\\partial L}{\\partial b}$$\n","\n","Where $\\eta$ is a given value called the *learning rate*."]},{"cell_type":"code","metadata":{"id":"gZvbMRo_DqpU"},"source":["\"\"\"\n","The sigmoid function\n","\"\"\"\n","def sigmoid(valeur):\n","  #TODO\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-ZgyF4xDh5J"},"source":["class Neuron:\n","\n","  \"\"\"\n","  Build of a perceptron\n","  weights : List of weights\n","  bias : The bias.\n","  \"\"\"\n","  def __init__(self, weights, bias):\n","    self.weights = weights\n","    self.bias = bias\n","\n","\n","  \"\"\"\n","  Function called when you want to get the output from the input\n","  input : List of input values.\n","  \"\"\"\n","  def forward(self, input):\n","    assert(len(input) == len(self.weights))\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","  Function to compute the delta value\n","  \"\"\"\n","  def delta(self, predicted_output, expected_output, input):\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","  Updates the weights and bias\n","  \"\"\"\n","  def backward(self, input, delta):\n","    learning_rate = 0.1\n","    #TODO\n","    ...\n","    \n","  \"\"\"\n","  Computes the output from the input using our neuron.\n","  \"\"\"\n","  def predict(self, input):\n","    return int(round(self.forward(input)))\n","    \n","  \"\"\"\n","  Computes the accuracy of our neuron.\n","  inputs: List of List of inputs\n","  outputs: list of the expected output values.\n","  \"\"\"\n","  def accuracy(self, inputs, outputs):\n","    #TODO\n","    ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKwTrsC067de"},"source":["Now that you have programmed a **sigmoid neuron**, you can use it! Let's try to allow this neuron to **learn** to reproduce the behavior of an OR gate.\n","This time, you don't have to look for the right weights and biases yourself. They will be learned automatically."]},{"cell_type":"code","metadata":{"id":"cUsMUdNjDqRT"},"source":["# Example for a OR gate:\n","\n","or_neuron = Neuron([random(),random()], random()) # Build a neuron with random weights and bias\n","or_input = [[1,1], [1,0], [0,1], [0,0]] # List of possible inputs for the neuron \n","or_output = [1, 1, 1, 0] # List of the expected outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-OTlGHN70p5"},"source":["##Training the Neuron\n","\n","Now that we have created the neuron, we will have to train it!  \n","So we have to make a loop that trains the neuron enough to predict all the outputs correctly. We call **epochs** the number of times we will go through this loop. One **epoch** corresponds to one training on all the data. So, to train on 10 epochs is training the neuron 10 times on the 4 inputs data.\n","For each epoch we need:\n","\n","1.   Run the data through the neural network\n","2.   Compute the loss\n","3.   Backpropagate the loss\n","\n","In order to be able to follow the evolution of our neuron, calculate, at each end of loop, the accuracy and save it somewhere. You can then draw a graph to see if the accuracy of your neuron is increasing over time."]},{"cell_type":"code","metadata":{"id":"j-H4_IHIDxOG"},"source":["# Training\n","accuracy = []\n","nb_epochs = 2000\n","for j in range(nb_epochs):\n","  #TODO\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWO3euAwvKPk"},"source":["If you are here, it means that you have trained a neuron to reproduce an OR gate! Check by yourself the outputs"]},{"cell_type":"code","metadata":{"id":"qcdPuK2aDzyW"},"source":["for i in range(4):\n","  print(\"Predicted : \"+str(or_neuron.predict(or_input[i]))+\" Expected ouput : \"+str(or_output[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCFUU8GlvkR7"},"source":["Using plt.plot, check how the accuracy has evolved"]},{"cell_type":"code","metadata":{"id":"upmWxqWEB36O"},"source":["plt.plot(accuracy)\n","plt.xlabel(\"nb of epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4WlTR6Ky0eG"},"source":["\n","To understand what exactly our little neuron does, we will draw its decision line.\n","This line corresponds to the case $y = \\frac{1}{2}$ where $y$ represents the output for a ($x_2$,$x_1$).\n","\n","$ y = \\frac{1}{2} \\iff x_2w_2 + x_1w_1 + b = 0 \\iff x_2 = \\frac{-x_1w_1 - b}{w_2}$\n","\n","\n","To draw the decision line, we have to draw $f$ where $f(x) = \\frac{-w_1x - b}{w_2}$\n"]},{"cell_type":"code","metadata":{"id":"CkArKaWjqQwh"},"source":["x = linspace(-0.1, 1.1, 30)\n","\n","y = -or_neuron.weights[0] / or_neuron.weights[1] * x - or_neuron.bias / or_neuron.weights[1]\n","\n","plt.figure(figsize=(8,8))\n","plt.plot(x, y)\n","plt.xlim(-0.1, 1.1)\n","plt.ylim(-0.1, 1.1)\n","plt.scatter([0,1,1], [1,0,1], c = 'red')\n","plt.scatter([0], [0], c = 'blue')\n","plt.title(\"OR gate decision boundary\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgyOktw068cX"},"source":["And if you now try to train a sigmoid neuron to reproduce the behavior of an XOR gate...?"]},{"cell_type":"code","metadata":{"id":"VfzoukL2D1lo"},"source":["# CORRECTION: It is not possible to have ONE sigmoid neuron that learns to reproduce an XOR gate\n","xor_neuron = Neuron([randint(0,20),randint(0,20)], randint(-20,-15))\n","xor_input = [[1,1], [1,0], [0,1], [0,0]]\n","xor_output = [0, 1, 1, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxUOR_wLD3wp"},"source":["# Training\n","epochs_xor = 3000\n","accuracy_xor = []\n","for i in range(epochs_xor):\n","  #TODO\n","  ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what we have after the training"],"metadata":{"id":"As18NzXR5CLr"}},{"cell_type":"code","metadata":{"id":"e-B6kgMBtBBy"},"source":["# Decision line\n","x = linspace(-0.1, 1.1, 30)\n","y = - xor_neuron.weights[0] / xor_neuron.weights[1] * x - xor_neuron.bias / xor_neuron.weights[1]\n","\n","plt.figure(figsize=(8, 8))\n","plt.plot(x, y)\n","plt.xlim(-0.1, 1.1)\n","plt.ylim(-0.1, 1.1)\n","plt.scatter([0,1], [0,1], c = 'red')\n","plt.scatter([0,1], [1,0], c = 'blue')\n","plt.title(\"XOR gate decision boundary\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj5hUns2Qbe-"},"source":["#plot of the accuracy\n","x = range(epochs_xor)\n","y = accuracy_xor\n","plt.plot(x, y)\n","plt.xlabel(\"nb of epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll see next time how to make a XOR gate... And even more !"],"metadata":{"id":"41CePlPf6cJH"}}]}